diff -ruN linux/arch/x86/include/asm/page_64_types.h archlinux-linux/arch/x86/include/asm/page_64_types.h
--- linux/arch/x86/include/asm/page_64_types.h	2022-03-23 17:39:00.242183791 +0200
+++ archlinux-linux/arch/x86/include/asm/page_64_types.h	2023-02-08 13:23:39.824091737 +0200
@@ -94,8 +94,15 @@
 #define TASK_SIZE_OF(child)	((test_tsk_thread_flag(child, TIF_ADDR32)) ? \
 					IA32_PAGE_OFFSET : TASK_SIZE_MAX)
 
+#ifdef CONFIG_KROOM
+#define __VIRTUAL_SAMPLED_MIN	(((_AC(1,UL) << (__VIRTUAL_MASK_SHIFT))) - (_AC(1,UL) << (__VIRTUAL_MASK_SHIFT - 8)))
+#define __VIRTUAL_UNSAMPLED_MAX	(__VIRTUAL_SAMPLED_MIN - PAGE_SIZE)
+#define STACK_TOP		((TASK_SIZE_LOW > __VIRTUAL_UNSAMPLED_MAX)?__VIRTUAL_UNSAMPLED_MAX:TASK_SIZE_LOW)
+#define STACK_TOP_MAX		((TASK_SIZE_MAX > __VIRTUAL_UNSAMPLED_MAX)?__VIRTUAL_UNSAMPLED_MAX:TASK_SIZE_MAX)
+#else
 #define STACK_TOP		TASK_SIZE_LOW
 #define STACK_TOP_MAX		TASK_SIZE_MAX
+#endif /* CONFIG_KROOM */
 
 /*
  * In spite of the name, KERNEL_IMAGE_SIZE is a limit on the maximum virtual
diff -ruN linux/arch/x86/include/asm/pgalloc.h archlinux-linux/arch/x86/include/asm/pgalloc.h
--- linux/arch/x86/include/asm/pgalloc.h	2022-03-23 17:39:00.242183791 +0200
+++ archlinux-linux/arch/x86/include/asm/pgalloc.h	2023-02-08 13:23:34.084065786 +0200
@@ -29,6 +29,10 @@
 static inline void paravirt_release_p4d(unsigned long pfn) {}
 #endif
 
+#ifdef CONFIG_KROOM
+#include <asm/sampler.h>
+#endif
+
 /*
  * Flags to use when allocating a user page table page.
  */
@@ -116,6 +120,19 @@
 {
 	paravirt_alloc_pud(mm, __pa(pud) >> PAGE_SHIFT);
 	set_p4d(p4d, __p4d(_PAGE_TABLE | __pa(pud)));
+	#ifdef CONFIG_KROOM
+	#if CONFIG_PGTABLE_LEVELS > 4
+	if(!pgtable_l5_enabled() && (((long)(p4d)) & 0xff8) == 0x7f8){
+		pgfault_sampler_activate(mm);
+		printk(KERN_INFO "populating monitored p4d\n");
+	}
+	#else
+	if((((long)(p4d)) & 0xff8) == 0x7f8){
+		pgfault_sampler_activate(mm);
+		printk(KERN_INFO "populating monitored p4d\n");
+	}
+	#endif	/* CONFIG_PGTABLE_LEVELS > 4 */
+	#endif /* CONFIG_KROOM */
 }
 
 static inline void p4d_populate_safe(struct mm_struct *mm, p4d_t *p4d, pud_t *pud)
@@ -139,6 +156,12 @@
 		return;
 	paravirt_alloc_p4d(mm, __pa(p4d) >> PAGE_SHIFT);
 	set_pgd(pgd, __pgd(_PAGE_TABLE | __pa(p4d)));
+	#ifdef CONFIG_KROOM
+	if((((long)(pgd)) & 0xff8) == 0x7f8){
+		pgfault_sampler_activate(mm);
+		printk(KERN_INFO "populating monitored pgd\n");
+	}
+	#endif /* CONFIG_KROOM */
 }
 
 static inline void pgd_populate_safe(struct mm_struct *mm, pgd_t *pgd, p4d_t *p4d)
diff -ruN linux/arch/x86/include/asm/pgtable.h archlinux-linux/arch/x86/include/asm/pgtable.h
--- linux/arch/x86/include/asm/pgtable.h	2022-03-23 17:39:00.242183791 +0200
+++ archlinux-linux/arch/x86/include/asm/pgtable.h	2023-02-08 13:24:11.644235486 +0200
@@ -903,6 +903,13 @@
 
 static inline int p4d_present(p4d_t p4d)
 {
+	#ifdef CONFIG_KROOM
+	#if CONFIG_PGTABLE_LEVELS > 4
+	return p4d_flags(p4d) & (_PAGE_PRESENT | _PAGE_USER);
+	#else
+	return p4d_flags(p4d) & (_PAGE_PRESENT | _PAGE_USER);
+	#endif	/* CONFIG_PGTABLE_LEVELS > 4 */
+	#endif /* CONFIG_KROOM */
 	return p4d_flags(p4d) & _PAGE_PRESENT;
 }
 
@@ -926,6 +933,23 @@
 
 	return (p4d_flags(p4d) & ~ignore_flags) != 0;
 }
+
+#ifdef CONFIG_KROOM
+static inline p4d_t p4d_sample_poison(p4d_t p4d)
+{
+	return native_make_p4d(native_p4d_val(p4d) & (~_PAGE_PRESENT));
+}
+
+static inline p4d_t p4d_sample_unpoison(p4d_t p4d)
+{
+	return native_make_p4d(native_p4d_val(p4d) | _PAGE_PRESENT);
+}
+
+static inline int p4d_sample_poisoned(p4d_t p4d)
+{
+	return ((p4d_flags(p4d) & (_PAGE_PRESENT | _PAGE_USER)) == _PAGE_USER);
+}
+#endif /* CONFIG_KROOM */
 #endif  /* CONFIG_PGTABLE_LEVELS > 3 */
 
 static inline unsigned long p4d_index(unsigned long address)
diff -ruN linux/arch/x86/include/asm/sampler.h archlinux-linux/arch/x86/include/asm/sampler.h
--- linux/arch/x86/include/asm/sampler.h	1970-01-01 02:00:00.000000000 +0200
+++ archlinux-linux/arch/x86/include/asm/sampler.h	2023-02-08 13:24:30.784321873 +0200
@@ -0,0 +1,18 @@
+#ifndef _ASM_X86_SAMPLER_H
+#ifdef CONFIG_KROOM
+
+#include <linux/mm_types.h>
+
+struct sampler_data;
+
+void __init sampler_init(void);
+
+void pgfault_sampler_activate(struct mm_struct *mm);
+void pgfault_sampler_deactivate(struct mm_struct *mm);
+
+void pgfault_sampler_poison(struct mm_struct *mm);
+void pgfault_sampler_unpoison(struct mm_struct *mm);
+void pgfault_sampler_report(struct mm_struct *mm, void *addr, void *pc);
+
+#endif /* CONFIG_KROOM */
+#endif /* _ASM_X86_SAMPLER_H */
diff -ruN linux/arch/x86/include/uapi/asm/mman.h archlinux-linux/arch/x86/include/uapi/asm/mman.h
--- linux/arch/x86/include/uapi/asm/mman.h	2022-03-23 17:39:00.252183833 +0200
+++ archlinux-linux/arch/x86/include/uapi/asm/mman.h	2022-03-23 18:19:41.523206450 +0200
@@ -4,6 +4,8 @@
 
 #define MAP_32BIT	0x40		/* only give out 32bit addresses */
 
+#define MAP_SAMPLED	0x80		/* make the mapping available for the profiling sampler */
+
 #ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS
 /*
  * Take the 4 protection key bits out of the vma->vm_flags
diff -ruN linux/arch/x86/Kconfig archlinux-linux/arch/x86/Kconfig
--- linux/arch/x86/Kconfig	2022-03-23 17:39:00.232183750 +0200
+++ archlinux-linux/arch/x86/Kconfig	2023-02-08 13:20:28.003221045 +0200
@@ -1851,6 +1851,17 @@
 
 	  If unsure, say y.
 
+config KROOM
+	prompt "Pagefault sampling based profiler (KROOM)"
+	def_bool n
+	# Note: only available in 64-bit mode
+	depends on X86_64
+	help
+	  Enable sample based monitoring of memory acceses for profiling reasons.
+	  It uses page faults in order to work. Necessary for SHAMBLES
+
+	  If unsure, say n.
+
 choice
 	prompt "TSX enable mode"
 	depends on CPU_SUP_INTEL
diff -ruN linux/arch/x86/kernel/sys_x86_64.c archlinux-linux/arch/x86/kernel/sys_x86_64.c
--- linux/arch/x86/kernel/sys_x86_64.c	2022-03-23 17:39:00.262183874 +0200
+++ archlinux-linux/arch/x86/kernel/sys_x86_64.c	2023-02-08 13:23:54.874159746 +0200
@@ -114,6 +114,22 @@
 		}
 		return;
 	}
+	
+	#ifdef CONFIG_KROOM
+	if (!in_32bit_syscall()) {
+		if(flags & MAP_SAMPLED) {
+			*begin = __VIRTUAL_SAMPLED_MIN;
+			*end = task_size_64bit(1);
+		}else{
+			*begin	= get_mmap_base(1);
+			*end = task_size_64bit(addr > DEFAULT_MAP_WINDOW);
+			if(*end > __VIRTUAL_UNSAMPLED_MAX){
+				*end = __VIRTUAL_UNSAMPLED_MAX;
+			}
+		}
+		return;
+	}
+	#endif /* CONFIG_KROOM */
 
 	*begin	= get_mmap_base(1);
 	if (in_32bit_syscall())
@@ -138,6 +154,24 @@
 
 	if (len > end)
 		return -ENOMEM;
+	
+	#ifdef CONFIG_KROOM
+	if(flags & MAP_SAMPLED) {
+		printk(KERN_INFO "__VIRTUAL_SAMPLED_MIN=%llx__VIRTUAL_UNSAMPLED_MAX%llx\n", __VIRTUAL_SAMPLED_MIN, __VIRTUAL_UNSAMPLED_MAX);
+	}
+	if (addr) {
+		addr = PAGE_ALIGN(addr);
+		if(flags & MAP_SAMPLED) {
+			if(addr < __VIRTUAL_SAMPLED_MIN){
+				addr = NULL;
+			}
+		}else{
+			if(addr + len > __VIRTUAL_UNSAMPLED_MAX){
+				addr = NULL;
+			}
+		}
+	}
+	#endif /* CONFIG_KROOM */
 
 	if (addr) {
 		addr = PAGE_ALIGN(addr);
@@ -185,6 +219,17 @@
 	/* requesting a specific address */
 	if (addr) {
 		addr &= PAGE_MASK;
+		#ifdef CONFIG_KROOM
+		if(flags & MAP_SAMPLED) {
+			if(addr < __VIRTUAL_SAMPLED_MIN){
+				goto get_unmapped_area;
+			}
+		}else{
+			if(addr + len > __VIRTUAL_UNSAMPLED_MAX){
+				goto get_unmapped_area;
+			}
+		}
+		#endif /* CONFIG_KROOM */
 		if (!mmap_address_hint_valid(addr, len))
 			goto get_unmapped_area;
 
@@ -209,8 +254,22 @@
 	if (addr > DEFAULT_MAP_WINDOW && !in_32bit_syscall())
 		info.high_limit += TASK_SIZE_MAX - DEFAULT_MAP_WINDOW;
 
-	info.align_mask = 0;
-	info.align_offset = pgoff << PAGE_SHIFT;
+	#ifdef CONFIG_KROOM
+        if(flags & MAP_SAMPLED) {
+                info.low_limit = __VIRTUAL_SAMPLED_MIN;
+                info.high_limit = task_size_64bit(1);
+        }else{
+                info.low_limit = PAGE_SIZE;
+                info.high_limit = get_mmap_base(0);
+                if(info.high_limit > __VIRTUAL_UNSAMPLED_MAX){
+                        info.high_limit = __VIRTUAL_UNSAMPLED_MAX;
+                }
+        }
+        #else
+        info.low_limit = PAGE_SIZE;
+        info.high_limit = get_mmap_base(0);
+        #endif /* CONFIG_KROOM */
+
 	if (filp) {
 		info.align_mask = get_align_mask();
 		info.align_offset += get_align_bits();
diff -ruN linux/arch/x86/mm/Makefile archlinux-linux/arch/x86/mm/Makefile
--- linux/arch/x86/mm/Makefile	2022-03-23 17:39:00.272183916 +0200
+++ archlinux-linux/arch/x86/mm/Makefile	2023-02-08 13:23:48.584131326 +0200
@@ -55,3 +55,5 @@
 obj-$(CONFIG_AMD_MEM_ENCRYPT)	+= mem_encrypt.o
 obj-$(CONFIG_AMD_MEM_ENCRYPT)	+= mem_encrypt_identity.o
 obj-$(CONFIG_AMD_MEM_ENCRYPT)	+= mem_encrypt_boot.o
+
+obj-$(CONFIG_KROOM)	+= sampler.o
diff -ruN linux/arch/x86/mm/sampler.c archlinux-linux/arch/x86/mm/sampler.c
--- linux/arch/x86/mm/sampler.c	1970-01-01 02:00:00.000000000 +0200
+++ archlinux-linux/arch/x86/mm/sampler.c	2022-07-04 12:15:05.448574249 +0300
@@ -0,0 +1,168 @@
+#include <asm/sampler.h>
+#include <asm/tlb.h>
+#include <linux/hrtimer.h>
+
+#include <linux/debugfs.h>
+#include <linux/fs.h>
+
+#define TIMER_SEC 0
+#define TIMER_NSEC 100000
+
+static ktime_t kt;
+
+struct sampler_data{
+	struct mutex		lock;
+	struct semaphore	wait_sem;
+	void 				*addr, *pc;
+};
+
+static struct sampler_data *sampler_data_create(void){
+	struct sampler_data *data;
+	data = kmalloc(sizeof(struct sampler_data), GFP_KERNEL);
+	mutex_init(&data->lock);
+	sema_init(&data->wait_sem, 0);
+	data->addr = NULL;
+	data->pc = NULL;
+	return data;
+}
+
+static void sampler_data_destroy(struct sampler_data *data){
+
+}
+
+
+static enum hrtimer_restart sampler_timer_hander(struct hrtimer *timer)
+{
+	struct mm_struct *mm;
+	mm = container_of(timer, struct mm_struct, sample_timer);
+	mmap_write_lock(mm);
+    pgfault_sampler_poison(mm);
+	mmap_write_unlock(mm);
+    hrtimer_forward(timer,timer->base->get_time(),kt);
+    return HRTIMER_RESTART;
+}
+
+void pgfault_sampler_activate(struct mm_struct *mm){
+	printk(KERN_INFO "Activating sampler for mmstruct %p\n", mm);
+	mm->sample_data = (void *)sampler_data_create();
+	hrtimer_init(&mm->sample_timer,CLOCK_MONOTONIC,HRTIMER_MODE_REL);
+	mm->sample_timer.function = sampler_timer_hander;
+	hrtimer_start(&mm->sample_timer,kt,HRTIMER_MODE_REL);
+}
+
+void pgfault_sampler_deactivate(struct mm_struct *mm){
+	printk(KERN_INFO "Deactivating sampler for mmstruct %p\n", mm);
+	hrtimer_cancel(&mm->sample_timer);
+	mm->sample_timer.function = NULL;
+	pgfault_sampler_unpoison(mm);
+}
+
+void pgfault_sampler_poison(struct mm_struct *mm){
+	struct mmu_gather tlb;
+	tlb_gather_mmu(&tlb, mm);
+	pgd_t *pgd;
+	p4d_t *p4d;
+	#if CONFIG_PGTABLE_LEVELS > 4
+	if(!pgtable_l5_enabled()){
+		pgd = pgd_offset(mm, 0x7f8000000000);
+		p4d = p4d_offset(pgd, 0x7f8000000000);
+	}else{
+		//todo
+	}
+	#else
+	pgd = pgd_offset(mm, 0x7f8000000000);
+	p4d = p4d_offset(pgd, 0x7f8000000000);
+	#endif	/* CONFIG_PGTABLE_LEVELS > 4 */
+	set_p4d(p4d, p4d_sample_poison(*p4d));
+	tlb_flush(&tlb);
+	tlb_finish_mmu(&tlb);
+	//printk(KERN_INFO "Timer hit\n");
+}
+
+void pgfault_sampler_unpoison(struct mm_struct *mm){
+	pgd_t *pgd;
+	p4d_t *p4d;
+	#if CONFIG_PGTABLE_LEVELS > 4
+	if(!pgtable_l5_enabled()){
+		pgd = pgd_offset(mm, 0x7f8000000000);
+		p4d = p4d_offset(pgd, 0x7f8000000000);
+	}else{
+		//todo
+	}
+	#else
+	pgd = pgd_offset(mm, 0x7f8000000000);
+	p4d = p4d_offset(pgd, 0x7f8000000000);
+	#endif	/* CONFIG_PGTABLE_LEVELS > 4 */
+	set_p4d(p4d, p4d_sample_unpoison(*p4d));
+	//printk(KERN_INFO "unpoisoned something\n");
+}
+
+void pgfault_sampler_report(struct mm_struct *mm, void *addr, void *pc){
+	struct sampler_data *data;
+	data = (struct sampler_data *)(mm->sample_data);
+	mutex_lock(&data->lock);
+
+	/*Hack alert make sure that the any unread old value is discarded*/
+	down_trylock(&data->wait_sem);
+
+	data->addr = addr;
+	data->pc = pc;
+	up(&data->wait_sem);
+	mutex_unlock(&data->lock);
+}
+
+/*
+ * Debugfs interface
+ */
+
+/*
+ * To user comunication
+ */
+
+static ssize_t sample_read(struct file *filp, char __user *buffer, size_t size, loff_t *offset){
+	struct sampler_data *data;
+	if(size != 16){
+		return -EINVAL;
+	}
+	data = (struct sampler_data *)current->mm->sample_data;
+	if(data == NULL){
+		return -EAGAIN;
+	}
+	if(down_interruptible(&data->wait_sem)){
+		return -ERESTARTSYS;
+	}
+	mutex_lock(&data->lock);
+
+	/*Hack alert make sure that you do not read a duplicate value*/
+	down_trylock(&data->wait_sem);
+
+	if(copy_to_user(buffer, &data->addr, 8)){
+		mutex_unlock(&data->lock);
+		return -EFAULT;
+	}
+	if(copy_to_user(buffer+8, &data->pc, 8)){
+		mutex_unlock(&data->lock);
+		return -EFAULT;
+	}
+
+	mutex_unlock(&data->lock);
+	return 16;
+}
+
+static struct file_operations fops = {
+	.owner			= THIS_MODULE,
+	.read			= sample_read
+};
+
+/*
+ * init
+ */
+void __init sampler_init(){
+	static struct dentry *dir = 0;
+
+	printk(KERN_INFO "Initing sampler\n");
+	kt = ktime_set(TIMER_SEC, TIMER_NSEC);
+	dir = debugfs_create_dir("sampler", NULL);
+	debugfs_create_x64("interval_ns", S_IRUGO|S_IWUGO, dir, &kt);
+	debugfs_create_file("samples", S_IRUGO|S_IWUGO, dir, NULL, &fops);
+}
diff -ruN linux/fs/debugfs/inode.c archlinux-linux/fs/debugfs/inode.c
--- linux/fs/debugfs/inode.c	2022-03-23 17:39:02.332192399 +0200
+++ archlinux-linux/fs/debugfs/inode.c	2023-02-08 13:24:01.344188973 +0200
@@ -30,6 +30,11 @@
 
 #include "internal.h"
 
+
+#ifdef CONFIG_KROOM
+#include <asm/sampler.h>
+#endif
+
 #define DEBUGFS_DEFAULT_MODE	0700
 
 static struct vfsmount *debugfs_mount;
@@ -846,6 +851,9 @@
 	else
 		debugfs_registered = true;
 
+#ifdef CONFIG_KROOM
+	sampler_init();
+#endif
 	return retval;
 }
 core_initcall(debugfs_init);
diff -ruN linux/include/linux/mm_types.h archlinux-linux/include/linux/mm_types.h
--- linux/include/linux/mm_types.h	2022-03-23 17:39:02.522193183 +0200
+++ archlinux-linux/include/linux/mm_types.h	2023-02-08 13:23:22.274012380 +0200
@@ -16,6 +16,9 @@
 #include <linux/workqueue.h>
 #include <linux/seqlock.h>
 
+#include <linux/hrtimer.h>
+#ifdef CONFIG_KROOM
+#endif
 #include <asm/mmu.h>
 
 #ifndef AT_VECTOR_SIZE_ARCH
@@ -577,6 +580,11 @@
 #endif
 	} __randomize_layout;
 
+#ifdef CONFIG_KROOM
+	struct hrtimer sample_timer;
+	void *sample_data;
+#endif
+
 	/*
 	 * The mm_cpumask needs to be at the end of mm_struct, because it
 	 * is dynamically sized based on nr_cpu_ids.
diff -ruN linux/mm/memory.c archlinux-linux/mm/memory.c
--- linux/mm/memory.c	2022-03-23 17:39:02.752194130 +0200
+++ archlinux-linux/mm/memory.c	2023-02-08 13:23:11.313962793 +0200
@@ -83,6 +83,10 @@
 #include <asm/tlb.h>
 #include <asm/tlbflush.h>
 
+#ifdef CONFIG_KROOM
+#include <asm/sampler.h>
+#endif /* CONFIG_KROOM */
+
 #include "pgalloc-track.h"
 #include "internal.h"
 
@@ -296,6 +300,12 @@
 		return;
 
 	pud = pud_offset(p4d, start);
+	#ifdef CONFIG_KROOM
+	if((((long)(p4d)) & 0xff8) == 0x7f8){
+		pgfault_sampler_deactivate(tlb->mm);
+		printk(KERN_INFO "clearing monitored p4d\n");
+	}
+	#endif /* CONFIG_KROOM */
 	p4d_clear(p4d);
 	pud_free_tlb(tlb, pud, start);
 	mm_dec_nr_puds(tlb->mm);
@@ -4433,6 +4443,18 @@
 	vmf.pud = pud_alloc(mm, p4d, address);
 	if (!vmf.pud)
 		return VM_FAULT_OOM;
+	
+	#ifdef CONFIG_KROOM
+	if(p4d_sample_poisoned(*p4d)){
+		pgfault_sampler_unpoison(mm);
+		pgfault_sampler_report(mm, address, KSTK_EIP(current));
+		return VM_FAULT_NOPAGE;
+	}
+	#endif /* CONFIG_KROOM */	
+	
+	//if(address >= 0x7f8000000000LLU){
+	//	printk(KERN_INFO "Faulting in addr=0x%lx, pgd_index=0x%lx, pgd=%lx, p4d=%lx, pud=%lx\n", address, pgd_index(address), pgd, p4d, vmf.pud);
+	//}
 retry_pud:
 	if (pud_none(*vmf.pud) && __transparent_hugepage_enabled(vma)) {
 		ret = create_huge_pud(&vmf);
diff -ruN linux/mm/mmap.c archlinux-linux/mm/mmap.c
--- linux/mm/mmap.c	2022-03-23 17:39:02.752194130 +0200
+++ archlinux-linux/mm/mmap.c	2023-02-08 13:23:28.284039561 +0200
@@ -1583,6 +1583,11 @@
 	    ((vm_flags & VM_LOCKED) ||
 	     (flags & (MAP_POPULATE | MAP_NONBLOCK)) == MAP_POPULATE))
 		*populate = len;
+	#ifdef CONFIG_KROOM
+	if(flags & MAP_SAMPLED){
+		printk(KERN_INFO "Alloced monitored addr %llx-%llx\n", addr, addr+len);
+	}
+	#endif
 	return addr;
 }
 
diff -ruN linux/mm/pgtable-generic.c archlinux-linux/mm/pgtable-generic.c
--- linux/mm/pgtable-generic.c	2022-03-23 17:39:02.752194130 +0200
+++ archlinux-linux/mm/pgtable-generic.c	2022-03-23 17:42:33.813063427 +0200
@@ -27,6 +27,7 @@
 #ifndef __PAGETABLE_P4D_FOLDED
 void p4d_clear_bad(p4d_t *p4d)
 {
+	dump_stack();
 	p4d_ERROR(*p4d);
 	p4d_clear(p4d);
 }
